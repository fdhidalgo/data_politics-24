---
title: "Surveys in Politics"
subtitle: "Lecture 13"
date: last-modified
author: 
  - name: F. Daniel Hidalgo
    email: dhidalgo@mit.edu
    affiliations: MIT
format: 
  clean-revealjs:
    incremental: true
engine: knitr
execute: 
  cache: true
webr:
  packages: ['tidyverse'] # Install R packages on document open
  show-startup-message: false
filters: 
  - timer
  - webr
bibliography: ../data_politics_2024.bib
editor:
  render-on-save: true
---

```{r}
#| label: setup
#| echo: false
#| warning: false
#| cache: false

library(tidyverse)
library(hrbrthemes)
library(gganimate)
theme_set(theme_ipsum())

```

## The Survey Life Cycle

![](images/survey_life_cycle.png)

@grovesSurveyMethodology2009

## Sources of Error in Surveys

![](images/survey_life_cycle_measurement.png)

## Sources of Error in Surveys

![](images/survey_life_cycle_representation.png)

# Measurement Error

## Measurement Error

[**Key Point**]{.alert}: survey responses are subject to considerable error

  - For example, think about the process of answering the following two  questions:

. . .

  > On average, during the last 6 months, that is, since ______, how often have YOU gone shopping? For example, at drug, clothing, grocery, hardware, and convenience stores?

  > Now turning to business conditions in the country as a whole, do you think that the next 12 months we'll  have good times financially, or bad times, or what?
  
## Case study: Education

Let's think about a very simple and straightforward survey question:

> What is the highest level of education you have completed?
(1) No HS
(2) High school graduate
(3) Some college
(4) 2-year
(5) 4-year
(6) Post-grad

One might expect low error rates on something like this, but let's examine how people answer this question two years apart:
```{r}
#| label: education
#| echo: true
cces <- read_csv("cces_1012_panel.csv.gz", show_col_types = FALSE)
table(cces$educ_10, cces$educ_12)

```

## Problems with Survey Questions

::: columns
::: {.column width="50%"}

1. Unclear questions or excessive complexity

2. Forgetting:

3. Flawed estimation (over and under-reporting)

4. Problems in formatting an answer (open ended, close ended with ordered response scales, categorical responses)

5. Deliberate misreporting, i.e. partisan cheerleading


:::

::: {.column .fragment width="50%"}
![](images/recall.png)
:::
:::

## Partisan Cheerleading

```{r}
#| label: partisan_cheerleading
#| echo: false

bernanke_data <- data.frame(year = c(2006, 2007, 2008, 2009, 2006, 2007, 2008, 2009),
                            party = c(rep("Republicans", 4), rep("Democrats", 4)),
                            support = c(57, 59, 61, 36, 31, 40, 40, 64))

ggplot(bernanke_data, aes(x = year, y = support, color = party)) +
  geom_point(size = 3) +  scale_color_manual(values = c("blue", "red")) +
  geom_line(size = 2, aes(group = party)) +
  ylab("Confidence in Ben Bernanke") +
  theme_ipsum() +   theme(legend.position="bottom")

```

## Non-Citizen Voting?

- [Paper](https://doi.org/10.1016/j.electstud.2014.09.001) by Richman, Chattha, and Earnest (2014) use 2008 and 2010 Cooperative Congressional Election Studies (CCES) survey to study the prevalence of non-citizen voting.
- CCES have large samples: 32k in 2008 and 55k in 2010.
- Includes question about citizenship and turnout, which is checked against the voter file in most states.
-  20% in 2008 and 16% in 2010 of non-citizens claimed they were registered or had their registration status verified.
- 11% in 2008 and 4% in 2010 of non-citizens claimed they were registered or had their registration status verified.
- Using these estimates and accounting for sampling error, Richman et al  estimate that in 2008 between **38,000** and **2.8 million** non-citizens voted.

## Non-Citizen Voting?

![](images/trump_voting_tweet.jpg)

## Non-Citizen Voting?

```{webr-r}
url <- "http://dhidalgo.mit.edu/cces_1012_panel.csv.gz"
download.file(url, "cces.csv.gz")

```




## Voter Knowledge

- A central contribution of polling was to uncover how little the public knows about politics and government.
- Examples:
   - 40% knows which party controls congress.
   - 52% of voters know that a state has two senators.
   - 1/3 of people will express opinions on imaginary issues:
      - "Some people say that the 1975 Public Affairs Act should be repealed"
  - 34% knows the identity of the Supreme Court Chef Justice
  - 58% know Roe v Wade is about abortion

 
# Coverage Error

## Sample Frames

::: columns
::: {.column width="50%"}


Sampling Frames in use Today:

- No Sample frame, i.e. random digit dialing
  - Cell phones make this substantially more expensive
- Voter files
  - Available most of the country from government or vendors
- Internet samples
  - Recruited and opt-in


:::

::: {.column .fragment width="50%"}
*Question:* **What is your target population?**

![](images/frame_coverage.png)
:::
:::

## Coverage Bias

- Ignoring all sources of error except mismatch between frame and target population, coverage error is:

. . .

$$\bar Y_c - \bar Y = \frac{U}{N} (\bar Y_C- \bar Y_U)$$

- $\bar Y$: mean of the target population
- $\bar Y_c$: mean of the population covered by the sampling frame
- $\bar Y_U$: mean of the target population *not* covered by the sampling frame
- $U$: total number of eligible members not in the sampling frame
- $N$: total number of members in the target population


## Sampling Likely Voters 

- For campaigns, the target population is often people who will show up on election day, i.e. likely voters.

- Can ask whether people intend to vote, as well as past voting behavior to predict future turnout but people lie

. . .

> In talking to people about elections, we often find that a lot of people were not able to vote because they weren’t registered, they were sick, or they just didn’t have time. Which of the following statements best describes you?


- Over-reporting of voting is typically **8%** to **14%**
  



## Sampling Likely Voters 

Alternative question to increase accuracy:

. . .

> In talking to people about elections, we often find that a lot of people were not able to vote because they weren’t registered, they were sick, or they just didn’t have time. By looking at public records kept by election officials, we can get an accurate report of who actually voted in November, and in previous elections. Of course, these public records do not say who you voted for. Part of our study will involve checking these records against the survey reports. Which of the following statements best describes you?

Among those receiving the typical question, **11.8%** over-report. Among those receiving the alternative question, **8%** over-report

## Registration Based Sampling

- Past turnout is a very strong predictor of future turnout.
- As a result, campaigns increasingly use voter registration rolls (supplemented with commercial databases) to conduct surveys.
  - Intended turnout + past turnout + already registered is a much better predictor than intended turnout alone.
- Some media outlets now use registration-based sampling instead of random digit dialing.
  - BUT some populations unrepresented on voter lists.
  - Leads to biased view of American public opinion. How biased?

## Cambridge Voter File

```{r}
#| label: cambridge_voter_file
#| echo: true

voterfile_camb <- read_csv("cambridge_voter_history16_18.csv.gz", show_col_types = FALSE)

filter(voterfile_camb, turnout_090418 == 1) |>
  filter(last_name == "HIDALGO") |>
  select(last_name, first_name, date_of_birth, party_affiliation)

filter(voterfile_camb, turnout_110618 == 1) |>
  filter(last_name == "HIDALGO") |>
  select(last_name, first_name, date_of_birth, party_affiliation)

```

## Age and Turnout

```{r}
#| label: predictors_turnout
#| echo: false

voterfile_camb |>
  mutate(age = 2017 - year(date_of_birth)) |>
  group_by(age) |>
  summarize(turnout_110717 = 100 * mean(turnout_110717)) |>
  filter(age < 100) |> 
  ggplot(aes(x = age, y = turnout_110717)) +
  geom_point() +
  ylab("2017 Turnout")
  
```


## RDD vs Registration Based Sampling

![](images/pew_rbs_vs_rdd.png)


## Citations
