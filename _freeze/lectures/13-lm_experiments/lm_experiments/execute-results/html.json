{
  "hash": "5cdfb64d2bf43057128d5cce70709a5d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression and Experiments\"\nsubtitle: \"Lecture 13\"\ndate: last-modified\nauthor: \n  - name: F. Daniel Hidalgo\n    email: dhidalgo@mit.edu\n    affiliations: MIT\nformat: \n  clean-revealjs:\n    incremental: true\nengine: knitr\nexecute: \n  cache: true\nwebr:\n  packages: ['tidyverse'] # Install R packages on document open\n  show-startup-message: false\nfilters: \n  - timer\n  - webr\nbibliography: ../data_politics_2024.bib\neditor:\n  render-on-save: true\n---\n\n::: {.cell}\n\n:::\n\n\n## Models and Experiments\n\n- One of the benefits of randomized experiments is their simplicity:\n  - We can estimate causal effects using simple statistics like the difference-in-means that are easily communicated. \n  - Permutation tests give us a way to communicate uncertainty with very few assumptions. \n- But we can also use more complex models to estimate causal effects because:\n  - They can help us increase precision by accounting for more sources of variation.\n  - They can be used to examine heterogeneity \n  - They can be used to generate predictions\n- Running example: Effect of Voter Mobilization Messages in Spanish Media on Hispanic Turnout\n  \n## Linear Regression with a Binary Covariate\n\n\n::: {.cell}\n\n:::\n\n\nTo translate the difference-in-means into a regression model, we can use a linear regression with a binary covariate:\n\n$$\\texttt{hisp_to_2006}_i = \\alpha + \\beta_1 \\texttt{treatment} + \\epsilon_i$$\n\n- When independent variable is **binary** (like the treatment variable):\n  - Intercept $\\alpha$ is average value of the outcome when $\\texttt{treatment} = 0$.\n  - Coefficient $\\beta_1$ is the difference in the average outcome when $\\texttt{treatment} = 1$ compared to when $\\texttt{treatment} = 0$; i.e., the treatment effect.\n  \n## Linear Regression with a Binary Covariate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n\nlm_hisp <- lm(hisp_to_2006 ~ treatment, data = exp_data)\ntidy(lm_hisp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    32.1       2.07     15.5  5.65e-22\n2 treatment       7.01      3.94      1.78 8.04e- 2\n```\n\n\n:::\n\n```{.r .cell-code}\ngroup_by(exp_data, treatment) %>%\n  summarize(mean_hisp = mean(hisp_to_2006))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  treatment mean_hisp\n      <dbl>     <dbl>\n1         0      32.1\n2         1      39.2\n```\n\n\n:::\n:::\n\n\n## Linear Regression with a Categorical Variables\n\n- In this experiment, the treatment was randomized within 3 **blocks**. \n\n- As we learned earlier, one needs to \"analyze as ye randomize\", which means that we need to account for the blocking in our analysis.\n\n- We can do this by including the block as a categorical variable in the regression model:\n  - To include a categorical variable in a regression model, we create a set of binary variables that indicate the level of the categorical variable.\n\n. . .  \n  \n| Unit | Block | High | Medium | Low |\n|------|-------|------|--------|-----|\n| 1    | High  | 1    | 0      | 0   |\n| 2    | Medium| 0    | 1      | 0   |\n| 3    | Low   | 0    | 0      | 1   |\n\n## Linear Regression with a Categorical Variables\n\nThen we include **all but one** of these binary variables in the regression model:\n  $$ \\texttt{hisp_to_2006}_i = \\alpha + \\beta_1 \\texttt{treatment} + \\beta_2 \\texttt{block}_{\\texttt{1}} + \\beta_3 \\texttt{block}_{\\texttt{2}} + \\epsilon_i$$\n\n- The intercept $\\alpha$ is the average value of the outcome when $\\texttt{treatment} = 0$ and $\\texttt{block} = \\texttt{Low}$.\n- The coefficient $\\beta_1$ is the difference in the average outcome when $\\texttt{treatment} = 1$ compared to when $\\texttt{treatment} = 0$.\n- The coefficients $\\beta_2$ and $\\beta_3$ are the differences in the average outcome when $\\texttt{block} = \\texttt{High}$ and $\\texttt{block} = \\texttt{Medium}$ compared to when $\\texttt{block} = \\texttt{Low}$.\n\n## Linear Regression with a Categorical Variables\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_hisp_block <- lm(hisp_to_2006 ~ treatment + block, data = exp_data)\ntidy(lm_hisp_block)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   32.5        4.60     7.06  0.00000000333\n2 treatment      6.26       4.06     1.54  0.129        \n3 blockLow       4.15       6.67     0.622 0.536        \n4 blockMedium   -0.957      4.98    -0.192 0.848        \n```\n\n\n:::\n:::\n\n\n## Inference with the Bootstrap\n\n- We can use the bootstrap to estimate generate confidence intervals for coefficient estimates.\n- As with the difference-in-means test, we simply resample the data with replacement and estimate the model for each resample.\n- We can then use the distribution of the coefficient estimates to generate confidence intervals.\n\n## Population Regression\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_lm-1.png){width=960}\n:::\n:::\n\n\n## Randomly Sample from the Data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_lm1-1.png){width=960}\n:::\n:::\n\n\n## Randomly Sample from the Data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_lm2-1.png){width=960}\n:::\n:::\n\n\n## Randomly Sample from the Data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_lm3-1.png){width=960}\n:::\n:::\n\n\n## Bootstrap Distribution\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_dist-1.png){width=576}\n:::\n:::\n\n\n## Confidence Intervals for Treatment Effect\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nbs_lm <- function(data) {\n  lm(hisp_to_2006 ~ treatment + block, \n     data = sample_n(data, \n                     size = nrow(data), \n                     replace = TRUE)) |>\n    tidy() |>\n    filter(term == \"treatment\") |>\n    pull(estimate)\n}\n\nbs_lm_ci <- map(1:1000, ~bs_lm(exp_data)) |>\n  unlist()\n\n## Plot 95% confidence interval\nggplot() +\n  geom_histogram(aes(x = bs_lm_ci), bins = 30, alpha = 0.5) +\n  geom_vline(xintercept = c(quantile(bs_lm_ci, .025),\n                            quantile(bs_lm_ci, .975)),\n             color = \"red\") \n```\n\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/bootstrap_lm_ci-1.png){width=960}\n:::\n:::\n\n\n## Increasing Precision\n\n- We used a linear model to estimate the treatment effect, but the confidence interval is quite wide. \n- How can we use predictive modeling to increase the precision of our estimate?\n  - We can use predictive pre-treatment covariates to remove unexplained variation in the outcome. \n  - This will help us estimate the treatment effect more precisely by removing variation not related to the treatment.\n  \n. . . \n\nModel (ignoring block variables for now) with pre-treatment covariate:\n  \n$$ \\texttt{hisp_to_2006}_i = \\alpha + \\beta_1 \\texttt{treatment}  + \\beta_2 \\texttt{hisp_to_2002} + \\epsilon $$  \n\n## \"Adjusting\" for Pre-Treatment Covariates\n\n$$ \\texttt{hisp_to_2006}_i = \\alpha + \\beta_1 \\texttt{treatment}  + \\beta_2 \\texttt{hisp_to_2002} + \\epsilon $$  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(hisp_to_2006 ~ treatment + block, data = exp_data)  |>\n  tidy() |>\n  filter(term == \"treatment\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term      estimate std.error statistic p.value\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>\n1 treatment     6.26      4.06      1.54   0.129\n```\n\n\n:::\n\n```{.r .cell-code}\nlm(hisp_to_2006 ~ treatment + block + hisp_to_2002, data = exp_data)  |>\n  tidy() |>\n  filter(term == \"treatment\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term      estimate std.error statistic p.value\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>\n1 treatment     4.67      1.95      2.39  0.0203\n```\n\n\n:::\n:::\n\n\n## Adjusting for Pre-Treatment Covariates\n\n- Two models:\n  1. Short model: $Y_i = \\alpha + \\beta_1 X_i + \\epsilon_i$\n  2. Long model: $Y_i = \\alpha + \\beta_1 X_i + \\beta_2 Z_i + \\epsilon_i$\n\n\n- How $\\hat \\beta_1$ in the long model differ from $\\hat \\beta_1$ in the short model?\n  - $\\hat \\beta_1$ in the long model is the effect of $X_i$ on $Y_i$ after adjusting for $Z_i$.\n  \n- Mathematically, what does it mean to \"adjust\" for $Z_i$? Start with two models:\n 1.  $X_i  = \\gamma_0 + \\gamma_1 Z_i + u_i$ (regression of $X_i$ on $Z_i$)\n 2. $Y_i = \\delta_0 + \\delta_1 X_i + v_i$  (regression of $Y_i$ on $X_i$)\n\n## Partialling Out\n\n(1) $X_i  = \\gamma_0 + \\gamma_1 Z_i + u_i$ (regression of $X_i$ on $Z_i$)\n(2) $Y_i = \\delta_0 + \\delta_1 X_i + v_i$  (regression of $Y_i$ on $X_i$)\n\n. . .\n\nTo adjust for $Z_i$ when estimating the effect of $X_i$ on $Y_i$:\n\n1. Get the residuals from model 1: $\\hat u_i = X_i - \\hat \\gamma_0 - \\hat \\gamma_1 Z_i$ ($\\hat u_i$ is the part of $X_i$ that is not explained by $Z_i$)\n  \n2. Get the residuals from model 2: $\\hat v_i = Y_i - \\hat \\delta_0 - \\hat \\delta_1 X_i$ ($\\hat v_i$ is the part of $Y_i$ that is not explained by $X_i$)\n  \n3. Regress $\\hat v_i$ on $\\hat u_i$ to get the effect of $X_i$ on $Y_i$ after adjusting for $Z_i$.\n\n## Partialling Out in Experiments\n\n(1) $X_i  = \\gamma_0 + \\gamma_1 Z_i + u_i$ (regression of $X_i$ on $Z_i$)\n(2) $Y_i = \\delta_0 + \\delta_1 X_i + v_i$  (regression of $Y_i$ on $X_i$)\n\n- In experiments, $X_i$ is the treatment, $Z_i$ are pre-treatment covariates, and $Y_i$ is the outcome.\n- What should $\\gamma_1$ be in the regression of $X_i$ on $Z_i$? \n  -  $\\gamma_1$ should be zero because $Z_i$ should not be related to the treatment in an experiment.\n  - So in most cases, variation in $u_i$ is the same as variation in $X_i$.\n- $v_i$ however should be less variable than $Y_i$ because it removes the part of $Y_i$ that is not related to $X_i$.\n\n## Partialling Out in Practice\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nresiduals_yz <- lm(hisp_to_2006 ~ hisp_to_2002, data = exp_data) |>\n  residuals()\ndemeaned_y <- exp_data$hisp_to_2006 - mean(exp_data$hisp_to_2006)\n\nbind_rows(tibble(Y = demeaned_y, \n       Type = \"Unadjusted Y\"),\n          tibble(Y = residuals_yz,\n                 Type = \"Adjusted Y\")) |>\n  ggplot(aes(x = Y, fill = Type)) +\n  geom_density(alpha = .5) +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](lm_experiments_files/figure-revealjs/residuals-1.png){width=960}\n:::\n:::\n\n\n## Partialling Out in Practice\n\n::: {.cell}\n\n```{.r .cell-code}\nyx_resid <- lm(hisp_to_2006 ~ hisp_to_2002, data = exp_data) |>\n  residuals()\nxz_resid <- lm(treatment ~ hisp_to_2002, data = exp_data) |>\n  residuals()\n\nlm(yx_resid ~ xz_resid) |>\n  tidy() |>\n  filter(term == \"xz_resid\") |>\n  pull(estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.957391\n```\n\n\n:::\n\n```{.r .cell-code}\nmod <- lm(hisp_to_2006 ~ treatment + hisp_to_2002, data = exp_data)\ntidy(mod) |>\n  filter(term == \"treatment\") |>\n  pull(estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.957391\n```\n\n\n:::\n:::\n\n## Confidence Intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbs_lm_adj <- function(data) {\n  lm(hisp_to_2006 ~ treatment + block + hisp_to_2002,\n     data = sample_n(data, \n                     size = nrow(data), \n                     replace = TRUE)) |>\n    tidy() |>\n    filter(term == \"treatment\") |>\n    pull(estimate)\n}\n\nbs_lm_adj_ci <- map(1:1000, ~bs_lm_adj(exp_data)) |>\n  unlist()\n\n##Unadjusted Confidence Intervals\nquantile(bs_lm_ci, c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%     97.5% \n-1.401392 13.482128 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Adjusted Confidence Intervals\nquantile(bs_lm_adj_ci, c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%     97.5% \n0.4214788 9.0160956 \n```\n\n\n:::\n:::\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_1.png)\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_2.png)\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_3.png)\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_4.png)\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_5.png)\n\n## Alternative Perspective on Covariate Adjustment\n\n![](images/bad_draw_plot_6.png)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}